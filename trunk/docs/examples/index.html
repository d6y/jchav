This is initially written as plain unstyled html. I'll let someone who has a vague clue about fonts
do the css :)


<h2>Overview</h2>

<p>
This document will show how to use JChav to track performance of your deployed site over time.
</p>

<h2>Produce A JMeter Script</h2>
<p>
The first step is to produce a JMeter script that will exercise the site you wish to test. 
The Jakarta <a href="http://jakarta.apache.org/jmeter/">JMeter</a> web pages describe how to do this in depth.
JChav produces the top level set of images based upon the labels that you put into the JMeter configuration file. 
So where possible give your tasks meaningful names. This is especially important if you are using the http proxy tool built into JMeter to make test generation easy.
JChav will do its best to turn URLs into something meaningful, but if you take the trouble to set the name on the task the resulting pages will be better.
Make sure that the test plan you have created is running as you expect it to inside the JMeter workbench. When you are happy with this script save the script as a jmx file.</p>
</p>
<p>
We have included a small example called <a href="./digwalk.jmx">digwalk.jmx</a> which performs a series of simple calls to the <a href="http://digg.com">Digg</a> web site.
</p>

<h2>Running our script from ant</h2>
<p>
The good folks over at Programmer Planet have produced an ant task for running JMeter called <a href="http://www.programmerplanet.org/pages/projects/jmeter-ant-task.php">Jmeter Ant Task</a>.
Dowload the ant task and configure Jmeter as the describe. Confirm your task is running correctly inside ant paying particular attention to the information regarding the log format.
i.e.<b>jmeter.save.saveservice.output_format=xml</b>
</p>
<p>
The job of this ant task is to run our tests and produce an output file containing the results. But it is important that we make sure that we produce a different output file each time we perform our tests.
By default the task appends information to one file, but we want want file per run so that we can judge if the changes we have made to the software have improved or degraded performance over time.
</p>
<p>
To do this simply create a unique build id for each build performed. If you are using a continuous integration tool such as <a href="http://www.cruisecontrol.net">CruiseControl</a> then use the build id for the build, otherwise use something like a timestamp.
eg.
</p>
<p>
	 &lt;tstamp&gt;<br/>
            &lt;format property="build.id" pattern="dMMhhmmss" locale="en"/&gt;<br/>
     &lt;/tstamp&gt;<br/>
<br/>
<br/>        
	&lt;jmeter<br/>
   		 jmeterhome="c:\jakarta-jmeter-1.8.1"<br/>
    	 testplan="${basedir}/test/scripts/digwalk.jmx"<br/>
    	 resultlog="${basedir}/test/results/digresult{$build.id}.xml"/&gt;<br/>
</p>

<p>
Everytime we run the jmeter target we will create a file in the ${basedir}/test/results/ directory. This is the directory that we pass to the JChav task to produce our performance graphs.
</p>

<h2>Adding JChav to the build</h2>


// TODO
